{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChC3RF8meAlK"
   },
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
    "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
    "\n",
    "\n",
    "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
    "\n",
    "\n",
    "**Student Name**:s.Yahya Tehrani\n",
    "\n",
    "**Student ID**:400109265\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IraiR0SbeDi_"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRQjwWC3eDnc"
   },
   "source": [
    "**Task:** Implement your own Logistic Regression model, and test it on the given dataset of Logistic_question.csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class MyLogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(X_train.shape[1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, pred, target):\n",
    "        return nn.functional.binary_cross_entropy(pred, target)\n",
    "\n",
    "    def fit(self, train_loader, epochs, lr):\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            for data, target in train_loader:\n",
    "                pred = self(data.float())\n",
    "                loss = self.loss(pred, target.unsqueeze(1))\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        pred = self(test_data.float())\n",
    "        return pred.detach().numpy()\n",
    "\n",
    "class LogisticDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.data[idx, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-i-oubUlZ6e"
   },
   "source": [
    "**Task:** Test your model on the given dataset. You must split your data into train and test, with a 0.2 split, then normalize your data using X_train data. Finally, report 4 different evaluation metrics of the model on the test set. (You might want to first make the Target column binary!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KXzIy_2u-pG",
    "outputId": "9625f7e2-abb1-4591-c0fa-843525e0ffd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.875\n",
      "Precision: 1.0\n",
      "Recall: 0.875\n",
      "F1 Score: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Load the data from CSV\n",
    "data = pd.read_csv(\"Logistic_question.csv\").values\n",
    "\n",
    "\n",
    "X = data[:, :-1]  # Features\n",
    "y = data[:, -1]   # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "y_train_binary = (y_train > 0).astype(np.float32)\n",
    "y_test_binary = (y_test > 0).astype(np.float32)\n",
    "\n",
    "\n",
    "class LogisticDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.data[idx, :]), torch.from_numpy(np.array(self.targets[idx]))\n",
    "\n",
    "train_dataset = LogisticDataset(X_train, y_train_binary)\n",
    "test_dataset = LogisticDataset(X_test, y_test_binary)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "model = MyLogisticRegression()\n",
    "\n",
    "\n",
    "model.fit(train_loader, epochs=10, lr=0.01)\n",
    "\n",
    "\n",
    "predictions = model.predict(torch.from_numpy(X_test).float())\n",
    "predictions_binary = (predictions > 0.5).astype(np.float32)\n",
    "\n",
    "accuracy = accuracy_score(y_test_binary, predictions_binary)\n",
    "precision = precision_score(y_test_binary, predictions_binary)\n",
    "recall = recall_score(y_test_binary, predictions_binary)\n",
    "f1 = f1_score(y_test_binary, predictions_binary)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji0RXNGKv1pa"
   },
   "source": [
    "**Question:** What are each of your used evaluation metrics? And for each one, mention situations in which they convey more data on the model performance in specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldveD35twRRZ"
   },
   "source": [
    "**Your answer:**\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "Definition: Proportion of correct predictions out of the total predictions made by the model.\n",
    "Usage: Commonly used metric for evaluating classification models, especially when the dataset is balanced.\n",
    "Considerations: Can be misleading with imbalanced classes or when the costs of false positives and false negatives vary.\n",
    "\n",
    "Precision:\n",
    "\n",
    "Definition: Proportion of true positive predictions out of all positive predictions made by the model.\n",
    "Usage: Useful when the cost of false positives is high, such as in spam detection scenarios.\n",
    "Considerations: Particularly valuable in datasets with a high negative class imbalance and when minimizing false positives is critical.\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate):\n",
    "\n",
    "Definition: Proportion of true positive predictions out of all actual positive instances.\n",
    "Usage: Important when the cost of false negatives is high, like in medical diagnosis applications.\n",
    "Considerations: Beneficial in datasets with a high positive class imbalance and when reducing false negatives is crucial.\n",
    "\n",
    "F1 Score:\n",
    "\n",
    "Definition: Harmonic mean of Precision and Recall, offering a balance between the two metrics.\n",
    "Usage: Effective when the costs of false positives and false negatives are similar or when balancing Precision and Recall is necessary.\n",
    "Considerations: Valuable in imbalanced datasets where achieving a trade-off between Precision and Recall is desired.\n",
    "\n",
    "## Explanation of Evaluation Metrics\n",
    "\n",
    "### Accuracy\n",
    "Accuracy measures the proportion of correct predictions out of the total predictions made by the model. It is suitable for tasks where all classes are equally important and have roughly balanced class distributions. However, accuracy may not be an appropriate metric when classes are imbalanced, as it can be misleading in such cases.\n",
    "\n",
    "### Precision\n",
    "Precision measures the proportion of true positive predictions out of all positive predictions made by the model. It is particularly useful in tasks where minimizing false positives is critical, such as medical diagnosis or fraud detection. A high precision indicates that the model is making fewer false positive predictions, which is desirable in scenarios where false positives are costly or undesirable.\n",
    "\n",
    "### Recall (Sensitivity)\n",
    "Recall measures the proportion of true positive predictions out of all actual positive instances in the dataset. It is valuable in tasks where identifying all positive instances is crucial, even at the cost of higher false positives. For example, in disease detection, high recall ensures that as many true positive cases as possible are identified, even if it means some false positives are also classified.\n",
    "\n",
    "### F1 Score\n",
    "The F1 score is the harmonic mean of precision and recall, providing a balance between the two metrics. It is useful in tasks where there is an uneven class distribution or when both false positives and false negatives need to be minimized simultaneously. The F1 score is especially relevant in scenarios where there is a trade-off between precision and recall, and achieving a balance between them is essential for model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZCeRHZSw-mh"
   },
   "source": [
    "**Task:** Now test the built-in function of Python for Logistic Regression, and report all the same metrics used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Vb5lRSQXDLR3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9500\n",
      "Precision: 0.9459\n",
      "Recall: 1.0000\n",
      "F1-score: 0.9722\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"Logistic_question.csv\")\n",
    "\n",
    "# Make the Target column binary\n",
    "df['Target'] = df['Target'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('Target', axis=1).values\n",
    "y = df['Target'].values\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data using X_train data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate the logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCvIymmMy_ji"
   },
   "source": [
    "**Question:** Compare your function with the built-in function. On the matters of performance and parameters. Briefly explain what the parameters of the built-in function are and how they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY0ohM16z3De"
   },
   "source": [
    "**Your answer:**\n",
    "**Comparison between Custom Logistic Regression and Built-in Logistic Regression**\n",
    "\n",
    "**Performance**:\n",
    "- **Custom Logistic Regression**:\n",
    "    - Performance depends on optimization algorithm, loss function, and convergence criteria.\n",
    "    - Offers flexibility but may require more effort for optimization and tuning.\n",
    "- **Built-in Logistic Regression**:\n",
    "    - Provides a well-optimized and efficient implementation.\n",
    "    - Widely used in practice, optimized for speed and memory usage.\n",
    "\n",
    "**Parameters**:\n",
    "- **Custom Logistic Regression**:\n",
    "    - Adjustable parameters include learning rate, number of epochs, and optimization algorithm.\n",
    "- **Built-in Logistic Regression**:\n",
    "    - Parameters include *penalty*, *C*, *solver*, *max_iter*, *class_weight*, and *random_state*.\n",
    "    - *penalty*: Norm used in penalization.\n",
    "    - *C*: Inverse of regularization strength.\n",
    "    - *solver*: Optimization algorithm.\n",
    "    - *max_iter*: Maximum number of iterations.\n",
    "    - *class_weight*: Weights for handling class imbalance.\n",
    "    - *random_state*: Seed for random number generation.\n",
    "\n",
    "**Impact of Parameters**:\n",
    "- Choice of *solver* impacts convergence speed and performance, especially for large datasets.\n",
    "- *C* controls regularization strength; smaller values increase regularization to prevent overfitting.\n",
    "- *class_weight* helps address class imbalance by assigning different weights to classes.\n",
    "- Proper *random_state* ensures reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClMqoYlr2kr7"
   },
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukvlqDe52xP5"
   },
   "source": [
    "**Task:** Implement your own Multinomial Logistic Regression model. Your model must be able to handle any number of labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5Ir-_hFt286t"
   },
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegression:\n",
    "    def __init__(self, num_features, num_classes, learning_rate=0.01):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros((num_features, num_classes))\n",
    "        self.bias = np.zeros(num_classes)\n",
    "        \n",
    "    def softmax(self, logits):\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        logits = np.dot(X, self.weights) + self.bias\n",
    "        return self.softmax(logits)\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    \n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        m = X.shape[0]\n",
    "        grad_w = (1 / m) * np.dot(X.T, (y_pred - y_true))\n",
    "        grad_b = (1 / m) * np.sum(y_pred - y_true, axis=0)\n",
    "        return grad_w, grad_b\n",
    "    \n",
    "    def update_weights(self, grad_w, grad_b):\n",
    "        self.weights -= self.learning_rate * grad_w\n",
    "        self.bias -= self.learning_rate * grad_b\n",
    "        \n",
    "    def train(self, X, y, num_epochs=100):\n",
    "        for epoch in range(num_epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            grad_w, grad_b = self.backward(X, y, y_pred)\n",
    "            self.update_weights(grad_w, grad_b)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPQ3Rtay3Y2_"
   },
   "source": [
    "**Task:** Test your model on the given dataset. Do the same as the previous part, but here you might want to first make the Target column quantized into $i$ levels. Change $i$ from 2 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "9aP4QJPq29B3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization Level: 2\n",
      "Epoch 1/100, Loss: 0.6931471805599453\n",
      "Epoch 2/100, Loss: 0.6883537260772787\n",
      "Epoch 3/100, Loss: 0.6836098913756159\n",
      "Epoch 4/100, Loss: 0.6789150906536431\n",
      "Epoch 5/100, Loss: 0.6742687455046515\n",
      "Epoch 6/100, Loss: 0.6696702848248032\n",
      "Epoch 7/100, Loss: 0.6651191447203295\n",
      "Epoch 8/100, Loss: 0.6606147684139868\n",
      "Epoch 9/100, Loss: 0.6561566061510697\n",
      "Epoch 10/100, Loss: 0.6517441151052492\n",
      "Epoch 11/100, Loss: 0.6473767592844769\n",
      "Epoch 12/100, Loss: 0.643054009437169\n",
      "Epoch 13/100, Loss: 0.6387753429588612\n",
      "Epoch 14/100, Loss: 0.6345402437995029\n",
      "Epoch 15/100, Loss: 0.6303482023715372\n",
      "Epoch 16/100, Loss: 0.6261987154588955\n",
      "Epoch 17/100, Loss: 0.6220912861270183\n",
      "Epoch 18/100, Loss: 0.6180254236339952\n",
      "Epoch 19/100, Loss: 0.614000643342905\n",
      "Epoch 20/100, Loss: 0.610016466635422\n",
      "Epoch 21/100, Loss: 0.6060724208267405\n",
      "Epoch 22/100, Loss: 0.6021680390818658\n",
      "Epoch 23/100, Loss: 0.5983028603332952\n",
      "Epoch 24/100, Loss: 0.5944764292001227\n",
      "Epoch 25/100, Loss: 0.5906882959085751\n",
      "Epoch 26/100, Loss: 0.5869380162139913\n",
      "Epoch 27/100, Loss: 0.5832251513242463\n",
      "Epoch 28/100, Loss: 0.5795492678246136\n",
      "Epoch 29/100, Loss: 0.5759099376040597\n",
      "Epoch 30/100, Loss: 0.5723067377829549\n",
      "Epoch 31/100, Loss: 0.5687392506421848\n",
      "Epoch 32/100, Loss: 0.5652070635536393\n",
      "Epoch 33/100, Loss: 0.5617097689120583\n",
      "Epoch 34/100, Loss: 0.5582469640682068\n",
      "Epoch 35/100, Loss: 0.5548182512633515\n",
      "Epoch 36/100, Loss: 0.55142323756501\n",
      "Epoch 37/100, Loss: 0.5480615348039422\n",
      "Epoch 38/100, Loss: 0.5447327595123506\n",
      "Epoch 39/100, Loss: 0.5414365328632608\n",
      "Epoch 40/100, Loss: 0.5381724806110452\n",
      "Epoch 41/100, Loss: 0.5349402330330597\n",
      "Epoch 42/100, Loss: 0.5317394248723608\n",
      "Epoch 43/100, Loss: 0.5285696952814677\n",
      "Epoch 44/100, Loss: 0.5254306877671402\n",
      "Epoch 45/100, Loss: 0.5223220501361372\n",
      "Epoch 46/100, Loss: 0.5192434344419272\n",
      "Epoch 47/100, Loss: 0.5161944969323171\n",
      "Epoch 48/100, Loss: 0.513174897997971\n",
      "Epoch 49/100, Loss: 0.5101843021217886\n",
      "Epoch 50/100, Loss: 0.5072223778291148\n",
      "Epoch 51/100, Loss: 0.5042887976387537\n",
      "Epoch 52/100, Loss: 0.5013832380147566\n",
      "Epoch 53/100, Loss: 0.49850537931896355\n",
      "Epoch 54/100, Loss: 0.49565490576426824\n",
      "Epoch 55/100, Loss: 0.4928315053685857\n",
      "Epoch 56/100, Loss: 0.4900348699094968\n",
      "Epoch 57/100, Loss: 0.4872646948795505\n",
      "Epoch 58/100, Loss: 0.48452067944219956\n",
      "Epoch 59/100, Loss: 0.4818025263883522\n",
      "Epoch 60/100, Loss: 0.4791099420935173\n",
      "Epoch 61/100, Loss: 0.47644263647552787\n",
      "Epoch 62/100, Loss: 0.4738003229528229\n",
      "Epoch 63/100, Loss: 0.4711827184032713\n",
      "Epoch 64/100, Loss: 0.4685895431235229\n",
      "Epoch 65/100, Loss: 0.4660205207888713\n",
      "Epoch 66/100, Loss: 0.4634753784136116\n",
      "Epoch 67/100, Loss: 0.4609538463118854\n",
      "Epoch 68/100, Loss: 0.4584556580589922\n",
      "Epoch 69/100, Loss: 0.4559805504531635\n",
      "Epoch 70/100, Loss: 0.45352826347778086\n",
      "Epoch 71/100, Loss: 0.4510985402640319\n",
      "Epoch 72/100, Loss: 0.4486911270539927\n",
      "Epoch 73/100, Loss: 0.4463057731641257\n",
      "Epoch 74/100, Loss: 0.4439422309491855\n",
      "Epoch 75/100, Loss: 0.44160025576652373\n",
      "Epoch 76/100, Loss: 0.43927960594078475\n",
      "Epoch 77/100, Loss: 0.4369800427289848\n",
      "Epoch 78/100, Loss: 0.4347013302859667\n",
      "Epoch 79/100, Loss: 0.43244323563022535\n",
      "Epoch 80/100, Loss: 0.4302055286100945\n",
      "Epoch 81/100, Loss: 0.4279879818702928\n",
      "Epoch 82/100, Loss: 0.42579037081882076\n",
      "Epoch 83/100, Loss: 0.42361247359420273\n",
      "Epoch 84/100, Loss: 0.4214540710330734\n",
      "Epoch 85/100, Loss: 0.41931494663809826\n",
      "Epoch 86/100, Loss: 0.4171948865462278\n",
      "Epoch 87/100, Loss: 0.41509367949727993\n",
      "Epoch 88/100, Loss: 0.4130111168028467\n",
      "Epoch 89/100, Loss: 0.41094699231552145\n",
      "Epoch 90/100, Loss: 0.4089011023984431\n",
      "Epoch 91/100, Loss: 0.40687324589515483\n",
      "Epoch 92/100, Loss: 0.4048632240997728\n",
      "Epoch 93/100, Loss: 0.4028708407274628\n",
      "Epoch 94/100, Loss: 0.4008959018852213\n",
      "Epoch 95/100, Loss: 0.39893821604295987\n",
      "Epoch 96/100, Loss: 0.396997594004887\n",
      "Epoch 97/100, Loss: 0.3950738488811887\n",
      "Epoch 98/100, Loss: 0.3931667960600029\n",
      "Epoch 99/100, Loss: 0.3912762531796858\n",
      "Epoch 100/100, Loss: 0.3894020401013688\n",
      "Accuracy: 1.0\n",
      "==================================================\n",
      "Quantization Level: 3\n",
      "Epoch 1/100, Loss: 1.0986122886681096\n",
      "Epoch 2/100, Loss: 1.0927520340208712\n",
      "Epoch 3/100, Loss: 1.0869455120630551\n",
      "Epoch 4/100, Loss: 1.081191999916769\n",
      "Epoch 5/100, Loss: 1.0754907890165568\n",
      "Epoch 6/100, Loss: 1.069841184784572\n",
      "Epoch 7/100, Loss: 1.0642425063132495\n",
      "Epoch 8/100, Loss: 1.0586940860552763\n",
      "Epoch 9/100, Loss: 1.053195269520655\n",
      "Epoch 10/100, Loss: 1.0477454149806664\n",
      "Epoch 11/100, Loss: 1.0423438931785307\n",
      "Epoch 12/100, Loss: 1.036990087046577\n",
      "Epoch 13/100, Loss: 1.031683391429728\n",
      "Epoch 14/100, Loss: 1.0264232128151296\n",
      "Epoch 15/100, Loss: 1.0212089690677324\n",
      "Epoch 16/100, Loss: 1.0160400891716732\n",
      "Epoch 17/100, Loss: 1.0109160129772827\n",
      "Epoch 18/100, Loss: 1.0058361909535702\n",
      "Epoch 19/100, Loss: 1.0008000839460327\n",
      "Epoch 20/100, Loss: 0.9958071629396518\n",
      "Epoch 21/100, Loss: 0.9908569088269438\n",
      "Epoch 22/100, Loss: 0.9859488121809328\n",
      "Epoch 23/100, Loss: 0.9810823730329288\n",
      "Epoch 24/100, Loss: 0.9762571006549974\n",
      "Epoch 25/100, Loss: 0.9714725133470129\n",
      "Epoch 26/100, Loss: 0.9667281382281894\n",
      "Epoch 27/100, Loss: 0.9620235110329999\n",
      "Epoch 28/100, Loss: 0.9573581759113843\n",
      "Epoch 29/100, Loss: 0.9527316852331676\n",
      "Epoch 30/100, Loss: 0.9481435993965969\n",
      "Epoch 31/100, Loss: 0.943593486640931\n",
      "Epoch 32/100, Loss: 0.9390809228629962\n",
      "Epoch 33/100, Loss: 0.9346054914376463\n",
      "Epoch 34/100, Loss: 0.9301667830420568\n",
      "Epoch 35/100, Loss: 0.9257643954837868\n",
      "Epoch 36/100, Loss: 0.9213979335325488\n",
      "Epoch 37/100, Loss: 0.9170670087556279\n",
      "Epoch 38/100, Loss: 0.9127712393568883\n",
      "Epoch 39/100, Loss: 0.9085102500193172\n",
      "Epoch 40/100, Loss: 0.9042836717510465\n",
      "Epoch 41/100, Loss: 0.900091141734805\n",
      "Epoch 42/100, Loss: 0.8959323031807443\n",
      "Epoch 43/100, Loss: 0.891806805182593\n",
      "Epoch 44/100, Loss: 0.8877143025770848\n",
      "Epoch 45/100, Loss: 0.8836544558066173\n",
      "Epoch 46/100, Loss: 0.8796269307850866\n",
      "Epoch 47/100, Loss: 0.8756313987668577\n",
      "Epoch 48/100, Loss: 0.8716675362188164\n",
      "Epoch 49/100, Loss: 0.8677350246954617\n",
      "Epoch 50/100, Loss: 0.863833550716986\n",
      "Epoch 51/100, Loss: 0.859962805650303\n",
      "Epoch 52/100, Loss: 0.856122485592973\n",
      "Epoch 53/100, Loss: 0.8523122912599783\n",
      "Epoch 54/100, Loss: 0.8485319278733077\n",
      "Epoch 55/100, Loss: 0.844781105054297\n",
      "Epoch 56/100, Loss: 0.8410595367186868\n",
      "Epoch 57/100, Loss: 0.8373669409743426\n",
      "Epoch 58/100, Loss: 0.8337030400216001\n",
      "Epoch 59/100, Loss: 0.8300675600561828\n",
      "Epoch 60/100, Loss: 0.8264602311746477\n",
      "Epoch 61/100, Loss: 0.8228807872823156\n",
      "Epoch 62/100, Loss: 0.8193289660036325\n",
      "Epoch 63/100, Loss: 0.8158045085949258\n",
      "Epoch 64/100, Loss: 0.8123071598594986\n",
      "Epoch 65/100, Loss: 0.8088366680650253\n",
      "Epoch 66/100, Loss: 0.8053927848631954\n",
      "Epoch 67/100, Loss: 0.8019752652115628\n",
      "Epoch 68/100, Loss: 0.798583867297555\n",
      "Epoch 69/100, Loss: 0.7952183524645955\n",
      "Epoch 70/100, Loss: 0.7918784851402931\n",
      "Epoch 71/100, Loss: 0.7885640327666544\n",
      "Epoch 72/100, Loss: 0.7852747657322748\n",
      "Epoch 73/100, Loss: 0.7820104573064608\n",
      "Epoch 74/100, Loss: 0.7787708835752415\n",
      "Epoch 75/100, Loss: 0.7755558233792245\n",
      "Epoch 76/100, Loss: 0.7723650582532513\n",
      "Epoch 77/100, Loss: 0.7691983723678112\n",
      "Epoch 78/100, Loss: 0.7660555524721677\n",
      "Epoch 79/100, Loss: 0.7629363878391575\n",
      "Epoch 80/100, Loss: 0.7598406702116186\n",
      "Epoch 81/100, Loss: 0.7567681937504065\n",
      "Epoch 82/100, Loss: 0.7537187549839566\n",
      "Epoch 83/100, Loss: 0.7506921527593524\n",
      "Epoch 84/100, Loss: 0.7476881881948625\n",
      "Epoch 85/100, Loss: 0.7447066646338995\n",
      "Epoch 86/100, Loss: 0.7417473876003712\n",
      "Epoch 87/100, Loss: 0.7388101647553785\n",
      "Epoch 88/100, Loss: 0.7358948058552259\n",
      "Epoch 89/100, Loss: 0.7330011227107062\n",
      "Epoch 90/100, Loss: 0.7301289291476241\n",
      "Epoch 91/100, Loss: 0.7272780409685213\n",
      "Epoch 92/100, Loss: 0.7244482759155693\n",
      "Epoch 93/100, Loss: 0.7216394536345935\n",
      "Epoch 94/100, Loss: 0.7188513956401991\n",
      "Epoch 95/100, Loss: 0.716083925281959\n",
      "Epoch 96/100, Loss: 0.7133368677116371\n",
      "Epoch 97/100, Loss: 0.7106100498514122\n",
      "Epoch 98/100, Loss: 0.7079033003630716\n",
      "Epoch 99/100, Loss: 0.7052164496181454\n",
      "Epoch 100/100, Loss: 0.7025493296689493\n",
      "Accuracy: 0.8875\n",
      "==================================================\n",
      "Quantization Level: 4\n",
      "Epoch 1/100, Loss: 1.0986122886681096\n",
      "Epoch 2/100, Loss: 1.0890562128312073\n",
      "Epoch 3/100, Loss: 1.0797423568598932\n",
      "Epoch 4/100, Loss: 1.0706641905736387\n",
      "Epoch 5/100, Loss: 1.0618152626380186\n",
      "Epoch 6/100, Loss: 1.053189209987321\n",
      "Epoch 7/100, Loss: 1.0447797662442941\n",
      "Epoch 8/100, Loss: 1.0365807691679008\n",
      "Epoch 9/100, Loss: 1.0285861671657668\n",
      "Epoch 10/100, Loss: 1.020790024912712\n",
      "Epoch 11/100, Loss: 1.0131865281204235\n",
      "Epoch 12/100, Loss: 1.005769987506027\n",
      "Epoch 13/100, Loss: 0.9985348420091349\n",
      "Epoch 14/100, Loss: 0.991475661307982\n",
      "Epoch 15/100, Loss: 0.9845871476855846\n",
      "Epoch 16/100, Loss: 0.9778641372965847\n",
      "Epoch 17/100, Loss: 0.9713016008846388\n",
      "Epoch 18/100, Loss: 0.9648946439989754\n",
      "Epoch 19/100, Loss: 0.9586385067571491\n",
      "Epoch 20/100, Loss: 0.952528563199148\n",
      "Epoch 21/100, Loss: 0.9465603202759043\n",
      "Epoch 22/100, Loss: 0.9407294165130107\n",
      "Epoch 23/100, Loss: 0.9350316203880775\n",
      "Epoch 24/100, Loss: 0.9294628284577404\n",
      "Epoch 25/100, Loss: 0.9240190632678911\n",
      "Epoch 26/100, Loss: 0.918696471078257\n",
      "Epoch 27/100, Loss: 0.9134913194300758\n",
      "Epoch 28/100, Loss: 0.9083999945832598\n",
      "Epoch 29/100, Loss: 0.9034189988472079\n",
      "Epoch 30/100, Loss: 0.8985449478272493\n",
      "Epoch 31/100, Loss: 0.8937745676066534\n",
      "Epoch 32/100, Loss: 0.8891046918821868\n",
      "Epoch 33/100, Loss: 0.8845322590693652\n",
      "Epoch 34/100, Loss: 0.8800543093918408\n",
      "Epoch 35/100, Loss: 0.8756679819677504\n",
      "Epoch 36/100, Loss: 0.8713705119043789\n",
      "Epoch 37/100, Loss: 0.8671592274111127\n",
      "Epoch 38/100, Loss: 0.8630315469394022\n",
      "Epoch 39/100, Loss: 0.8589849763572882\n",
      "Epoch 40/100, Loss: 0.8550171061649948\n",
      "Epoch 41/100, Loss: 0.8511256087571308\n",
      "Epoch 42/100, Loss: 0.8473082357361614\n",
      "Epoch 43/100, Loss: 0.8435628152810342\n",
      "Epoch 44/100, Loss: 0.839887249574123\n",
      "Epoch 45/100, Loss: 0.8362795122890232\n",
      "Epoch 46/100, Loss: 0.8327376461411568\n",
      "Epoch 47/100, Loss: 0.8292597605026415\n",
      "Epoch 48/100, Loss: 0.8258440290824259\n",
      "Epoch 49/100, Loss: 0.8224886876722936\n",
      "Epoch 50/100, Loss: 0.8191920319589914\n",
      "Epoch 51/100, Loss: 0.8159524154024271\n",
      "Epoch 52/100, Loss: 0.8127682471796185\n",
      "Epoch 53/100, Loss: 0.8096379901938408\n",
      "Epoch 54/100, Loss: 0.8065601591482228\n",
      "Epoch 55/100, Loss: 0.8035333186828737\n",
      "Epoch 56/100, Loss: 0.800556081574476\n",
      "Epoch 57/100, Loss: 0.7976271069971634\n",
      "Epoch 58/100, Loss: 0.7947450988434015\n",
      "Epoch 59/100, Loss: 0.7919088041035101\n",
      "Epoch 60/100, Loss: 0.789117011302404\n",
      "Epoch 61/100, Loss: 0.786368548992077\n",
      "Epoch 62/100, Loss: 0.7836622842983214\n",
      "Epoch 63/100, Loss: 0.7809971215201499\n",
      "Epoch 64/100, Loss: 0.778372000780371\n",
      "Epoch 65/100, Loss: 0.7757858967257679\n",
      "Epoch 66/100, Loss: 0.773237817275327\n",
      "Epoch 67/100, Loss: 0.7707268024149765\n",
      "Epoch 68/100, Loss: 0.7682519230373058\n",
      "Epoch 69/100, Loss: 0.7658122798247575\n",
      "Epoch 70/100, Loss: 0.763407002174805\n",
      "Epoch 71/100, Loss: 0.7610352471656595\n",
      "Epoch 72/100, Loss: 0.7586961985610742\n",
      "Epoch 73/100, Loss: 0.7563890658528468\n",
      "Epoch 74/100, Loss: 0.7541130833396579\n",
      "Epoch 75/100, Loss: 0.7518675092409131\n",
      "Epoch 76/100, Loss: 0.7496516248442967\n",
      "Epoch 77/100, Loss: 0.7474647336857758\n",
      "Epoch 78/100, Loss: 0.7453061607608401\n",
      "Epoch 79/100, Loss: 0.7431752517657875\n",
      "Epoch 80/100, Loss: 0.7410713723679165\n",
      "Epoch 81/100, Loss: 0.7389939075035121\n",
      "Epoch 82/100, Loss: 0.7369422607025554\n",
      "Epoch 83/100, Loss: 0.7349158534391235\n",
      "Epoch 84/100, Loss: 0.7329141245064773\n",
      "Epoch 85/100, Loss: 0.7309365294158784\n",
      "Epoch 86/100, Loss: 0.7289825398182008\n",
      "Epoch 87/100, Loss: 0.7270516429474476\n",
      "Epoch 88/100, Loss: 0.7251433410853061\n",
      "Epoch 89/100, Loss: 0.7232571510459173\n",
      "Epoch 90/100, Loss: 0.7213926036800563\n",
      "Epoch 91/100, Loss: 0.7195492433979591\n",
      "Epoch 92/100, Loss: 0.717726627710057\n",
      "Epoch 93/100, Loss: 0.7159243267849098\n",
      "Epoch 94/100, Loss: 0.7141419230236546\n",
      "Epoch 95/100, Loss: 0.7123790106503176\n",
      "Epoch 96/100, Loss: 0.7106351953173589\n",
      "Epoch 97/100, Loss: 0.7089100937258459\n",
      "Epoch 98/100, Loss: 0.7072033332596782\n",
      "Epoch 99/100, Loss: 0.7055145516333043\n",
      "Epoch 100/100, Loss: 0.7038433965523987\n",
      "Accuracy: 0.8375\n",
      "==================================================\n",
      "Quantization Level: 5\n",
      "Epoch 1/100, Loss: 1.3862943611198906\n",
      "Epoch 2/100, Loss: 1.3746967330857736\n",
      "Epoch 3/100, Loss: 1.3633429550317548\n",
      "Epoch 4/100, Loss: 1.3522286210987038\n",
      "Epoch 5/100, Loss: 1.3413493170363768\n",
      "Epoch 6/100, Loss: 1.3307006261835854\n",
      "Epoch 7/100, Loss: 1.320278135213684\n",
      "Epoch 8/100, Loss: 1.3100774396374353\n",
      "Epoch 9/100, Loss: 1.3000941490566986\n",
      "Epoch 10/100, Loss: 1.29032389216375\n",
      "Epoch 11/100, Loss: 1.280762321482337\n",
      "Epoch 12/100, Loss: 1.2714051178478347\n",
      "Epoch 13/100, Loss: 1.2622479946250635\n",
      "Epoch 14/100, Loss: 1.2532867016634872\n",
      "Epoch 15/100, Loss: 1.2445170289906038\n",
      "Epoch 16/100, Loss: 1.2359348102453829\n",
      "Epoch 17/100, Loss: 1.2275359258545848\n",
      "Epoch 18/100, Loss: 1.2193163059557126\n",
      "Epoch 19/100, Loss: 1.2112719330712007\n",
      "Epoch 20/100, Loss: 1.2033988445392327\n",
      "Epoch 21/100, Loss: 1.195693134707289\n",
      "Epoch 22/100, Loss: 1.1881509568951707\n",
      "Epoch 23/100, Loss: 1.1807685251348237\n",
      "Epoch 24/100, Loss: 1.1735421156947654\n",
      "Epoch 25/100, Loss: 1.1664680683973647\n",
      "Epoch 26/100, Loss: 1.159542787737554\n",
      "Epoch 27/100, Loss: 1.1527627438118517\n",
      "Epoch 28/100, Loss: 1.1461244730667777\n",
      "Epoch 29/100, Loss: 1.1396245788758887\n",
      "Epoch 30/100, Loss: 1.1332597319547613\n",
      "Epoch 31/100, Loss: 1.1270266706232515\n",
      "Epoch 32/100, Loss: 1.1209222009243582\n",
      "Epoch 33/100, Loss: 1.114943196608921\n",
      "Epoch 34/100, Loss: 1.1090865989952639\n",
      "Epoch 35/100, Loss: 1.1033494167127338\n",
      "Epoch 36/100, Loss: 1.0977287253378782\n",
      "Epoch 37/100, Loss: 1.0922216669317684\n",
      "Epoch 38/100, Loss: 1.0868254494867187\n",
      "Epoch 39/100, Loss: 1.0815373462903637\n",
      "Epoch 40/100, Loss: 1.0763546952147518\n",
      "Epoch 41/100, Loss: 1.0712748979377937\n",
      "Epoch 42/100, Loss: 1.066295419104077\n",
      "Epoch 43/100, Loss: 1.0614137854317127\n",
      "Epoch 44/100, Loss: 1.0566275847715467\n",
      "Epoch 45/100, Loss: 1.0519344651247144\n",
      "Epoch 46/100, Loss: 1.0473321336241763\n",
      "Epoch 47/100, Loss: 1.0428183554855357\n",
      "Epoch 48/100, Loss: 1.0383909529320923\n",
      "Epoch 49/100, Loss: 1.0340478040987695\n",
      "Epoch 50/100, Loss: 1.0297868419192198\n",
      "Epoch 51/100, Loss: 1.0256060530001132\n",
      "Epoch 52/100, Loss: 1.0215034764862958\n",
      "Epoch 53/100, Loss: 1.017477202920239\n",
      "Epoch 54/100, Loss: 1.0135253730988976\n",
      "Epoch 55/100, Loss: 1.0096461769308498\n",
      "Epoch 56/100, Loss: 1.005837852296326\n",
      "Epoch 57/100, Loss: 1.0020986839125023\n",
      "Epoch 58/100, Loss: 0.9984270022062038\n",
      "Epoch 59/100, Loss: 0.9948211821959484\n",
      "Epoch 60/100, Loss: 0.9912796423850647\n",
      "Epoch 61/100, Loss: 0.9878008436674266\n",
      "Epoch 62/100, Loss: 0.9843832882471677\n",
      "Epoch 63/100, Loss: 0.9810255185735827\n",
      "Epoch 64/100, Loss: 0.9777261162922596\n",
      "Epoch 65/100, Loss: 0.9744837012133537\n",
      "Epoch 66/100, Loss: 0.9712969302977748\n",
      "Epoch 67/100, Loss: 0.9681644966619443\n",
      "Epoch 68/100, Loss: 0.9650851286016646\n",
      "Epoch 69/100, Loss: 0.9620575886355365\n",
      "Epoch 70/100, Loss: 0.9590806725682739\n",
      "Epoch 71/100, Loss: 0.9561532085741744\n",
      "Epoch 72/100, Loss: 0.9532740563009241\n",
      "Epoch 73/100, Loss: 0.9504421059938497\n",
      "Epoch 74/100, Loss: 0.9476562776406622\n",
      "Epoch 75/100, Loss: 0.9449155201366792\n",
      "Epoch 76/100, Loss: 0.9422188104704619\n",
      "Epoch 77/100, Loss: 0.9395651529297545\n",
      "Epoch 78/100, Loss: 0.9369535783275736\n",
      "Epoch 79/100, Loss: 0.9343831432482557\n",
      "Epoch 80/100, Loss: 0.9318529293132413\n",
      "Epoch 81/100, Loss: 0.9293620424663439\n",
      "Epoch 82/100, Loss: 0.9269096122782254\n",
      "Epoch 83/100, Loss: 0.9244947912697807\n",
      "Epoch 84/100, Loss: 0.9221167542541145\n",
      "Epoch 85/100, Loss: 0.9197746976967792\n",
      "Epoch 86/100, Loss: 0.9174678390939235\n",
      "Epoch 87/100, Loss: 0.9151954163679988\n",
      "Epoch 88/100, Loss: 0.9129566872806609\n",
      "Epoch 89/100, Loss: 0.9107509288624861\n",
      "Epoch 90/100, Loss: 0.9085774368591348\n",
      "Epoch 91/100, Loss: 0.9064355251935776\n",
      "Epoch 92/100, Loss: 0.9043245254440011\n",
      "Epoch 93/100, Loss: 0.902243786337014\n",
      "Epoch 94/100, Loss: 0.9001926732557697\n",
      "Epoch 95/100, Loss: 0.8981705677626242\n",
      "Epoch 96/100, Loss: 0.8961768671359536\n",
      "Epoch 97/100, Loss: 0.8942109839207543\n",
      "Epoch 98/100, Loss: 0.8922723454926562\n",
      "Epoch 99/100, Loss: 0.8903603936349821\n",
      "Epoch 100/100, Loss: 0.8884745841284941\n",
      "Accuracy: 0.8625\n",
      "==================================================\n",
      "Quantization Level: 6\n",
      "Epoch 1/100, Loss: 1.6094379124341\n",
      "Epoch 2/100, Loss: 1.5989930756581352\n",
      "Epoch 3/100, Loss: 1.5887219490191071\n",
      "Epoch 4/100, Loss: 1.5786219044765606\n",
      "Epoch 5/100, Loss: 1.5686903275196216\n",
      "Epoch 6/100, Loss: 1.5589246179084337\n",
      "Epoch 7/100, Loss: 1.5493221903667371\n",
      "Epoch 8/100, Loss: 1.5398804752338244\n",
      "Epoch 9/100, Loss: 1.5305969190832773\n",
      "Epoch 10/100, Loss: 1.5214689853150014\n",
      "Epoch 11/100, Loss: 1.5124941547262316\n",
      "Epoch 12/100, Loss: 1.5036699260663204\n",
      "Epoch 13/100, Loss: 1.4949938165793037\n",
      "Epoch 14/100, Loss: 1.486463362537442\n",
      "Epoch 15/100, Loss: 1.4780761197681955\n",
      "Epoch 16/100, Loss: 1.469829664176396\n",
      "Epoch 17/100, Loss: 1.4617215922627325\n",
      "Epoch 18/100, Loss: 1.4537495216390972\n",
      "Epoch 19/100, Loss: 1.445911091540816\n",
      "Epoch 20/100, Loss: 1.4382039633353334\n",
      "Epoch 21/100, Loss: 1.4306258210265377\n",
      "Epoch 22/100, Loss: 1.4231743717535816\n",
      "Epoch 23/100, Loss: 1.4158473462827907\n",
      "Epoch 24/100, Loss: 1.4086424994910456\n",
      "Epoch 25/100, Loss: 1.4015576108388754\n",
      "Epoch 26/100, Loss: 1.394590484831399\n",
      "Epoch 27/100, Loss: 1.3877389514652005\n",
      "Epoch 28/100, Loss: 1.3810008666592206\n",
      "Epoch 29/100, Loss: 1.3743741126677775\n",
      "Epoch 30/100, Loss: 1.3678565984738875\n",
      "Epoch 31/100, Loss: 1.3614462601611657\n",
      "Epoch 32/100, Loss: 1.3551410612626928\n",
      "Epoch 33/100, Loss: 1.3489389930853828\n",
      "Epoch 34/100, Loss: 1.3428380750085342\n",
      "Epoch 35/100, Loss: 1.3368363547554245\n",
      "Epoch 36/100, Loss: 1.330931908636972\n",
      "Epoch 37/100, Loss: 1.3251228417666803\n",
      "Epoch 38/100, Loss: 1.3194072882462553\n",
      "Epoch 39/100, Loss: 1.3137834113214653\n",
      "Epoch 40/100, Loss: 1.3082494035079955\n",
      "Epoch 41/100, Loss: 1.3028034866872216\n",
      "Epoch 42/100, Loss: 1.2974439121719779\n",
      "Epoch 43/100, Loss: 1.292168960742568\n",
      "Epoch 44/100, Loss: 1.2869769426533977\n",
      "Epoch 45/100, Loss: 1.2818661976107524\n",
      "Epoch 46/100, Loss: 1.2768350947223523\n",
      "Epoch 47/100, Loss: 1.2718820324194435\n",
      "Epoch 48/100, Loss: 1.2670054383522718\n",
      "Epoch 49/100, Loss: 1.262203769259871\n",
      "Epoch 50/100, Loss: 1.2574755108151816\n",
      "Epoch 51/100, Loss: 1.2528191774465636\n",
      "Epoch 52/100, Loss: 1.2482333121368339\n",
      "Epoch 53/100, Loss: 1.2437164862009884\n",
      "Epoch 54/100, Loss: 1.2392672990437998\n",
      "Epoch 55/100, Loss: 1.2348843778985137\n",
      "Epoch 56/100, Loss: 1.230566377547858\n",
      "Epoch 57/100, Loss: 1.2263119800286062\n",
      "Epoch 58/100, Loss: 1.2221198943209166\n",
      "Epoch 59/100, Loss: 1.217988856023657\n",
      "Epoch 60/100, Loss: 1.2139176270169274\n",
      "Epoch 61/100, Loss: 1.209904995112945\n",
      "Epoch 62/100, Loss: 1.2059497736964464\n",
      "Epoch 63/100, Loss: 1.2020508013557278\n",
      "Epoch 64/100, Loss: 1.198206941505409\n",
      "Epoch 65/100, Loss: 1.1944170820019622\n",
      "Epoch 66/100, Loss: 1.1906801347530223\n",
      "Epoch 67/100, Loss: 1.1869950353214431\n",
      "Epoch 68/100, Loss: 1.1833607425250197\n",
      "Epoch 69/100, Loss: 1.1797762380327637\n",
      "Epoch 70/100, Loss: 1.1762405259585613\n",
      "Epoch 71/100, Loss: 1.1727526324530098\n",
      "Epoch 72/100, Loss: 1.1693116052941772\n",
      "Epoch 73/100, Loss: 1.1659165134779856\n",
      "Epoch 74/100, Loss: 1.162566446808881\n",
      "Epoch 75/100, Loss: 1.159260515491399\n",
      "Epoch 76/100, Loss: 1.1559978497232049\n",
      "Epoch 77/100, Loss: 1.1527775992901366\n",
      "Epoch 78/100, Loss: 1.1495989331637422\n",
      "Epoch 79/100, Loss: 1.14646103910177\n",
      "Epoch 80/100, Loss: 1.143363123252022\n",
      "Epoch 81/100, Loss: 1.1403044097599542\n",
      "Epoch 82/100, Loss: 1.1372841403803704\n",
      "Epoch 83/100, Loss: 1.1343015740935214\n",
      "Epoch 84/100, Loss: 1.1313559867258938\n",
      "Epoch 85/100, Loss: 1.1284466705759393\n",
      "Epoch 86/100, Loss: 1.1255729340449727\n",
      "Epoch 87/100, Loss: 1.1227341012734324\n",
      "Epoch 88/100, Loss: 1.1199295117826793\n",
      "Epoch 89/100, Loss: 1.1171585201224843\n",
      "Epoch 90/100, Loss: 1.1144204955243298\n",
      "Epoch 91/100, Loss: 1.1117148215606338\n",
      "Epoch 92/100, Loss: 1.1090408958099824\n",
      "Epoch 93/100, Loss: 1.1063981295284429\n",
      "Epoch 94/100, Loss: 1.103785947327006\n",
      "Epoch 95/100, Loss: 1.1012037868551974\n",
      "Epoch 96/100, Loss: 1.0986510984908822\n",
      "Epoch 97/100, Loss: 1.0961273450362667\n",
      "Epoch 98/100, Loss: 1.0936320014201004\n",
      "Epoch 99/100, Loss: 1.0911645544060584\n",
      "Epoch 100/100, Loss: 1.088724502307287\n",
      "Accuracy: 0.675\n",
      "==================================================\n",
      "Quantization Level: 7\n",
      "Epoch 1/100, Loss: 1.6094379124341\n",
      "Epoch 2/100, Loss: 1.6015088632097794\n",
      "Epoch 3/100, Loss: 1.593713429081174\n",
      "Epoch 4/100, Loss: 1.586049237190596\n",
      "Epoch 5/100, Loss: 1.5785139467098503\n",
      "Epoch 6/100, Loss: 1.5711052488336037\n",
      "Epoch 7/100, Loss: 1.5638208667300824\n",
      "Epoch 8/100, Loss: 1.5566585554553014\n",
      "Epoch 9/100, Loss: 1.549616101836444\n",
      "Epoch 10/100, Loss: 1.5426913243294107\n",
      "Epoch 11/100, Loss: 1.5358820728550362\n",
      "Epoch 12/100, Loss: 1.5291862286179134\n",
      "Epoch 13/100, Loss: 1.5226017039112931\n",
      "Epoch 14/100, Loss: 1.516126441911046\n",
      "Epoch 15/100, Loss: 1.5097584164612434\n",
      "Epoch 16/100, Loss: 1.5034956318535166\n",
      "Epoch 17/100, Loss: 1.497336122601985\n",
      "Epoch 18/100, Loss: 1.49127795321521\n",
      "Epoch 19/100, Loss: 1.4853192179663337\n",
      "Epoch 20/100, Loss: 1.4794580406622935\n",
      "Epoch 21/100, Loss: 1.4736925744127667\n",
      "Epoch 22/100, Loss: 1.4680210013992947\n",
      "Epoch 23/100, Loss: 1.4624415326448532\n",
      "Epoch 24/100, Loss: 1.4569524077839913\n",
      "Epoch 25/100, Loss: 1.4515518948335253\n",
      "Epoch 26/100, Loss: 1.446238289963676\n",
      "Epoch 27/100, Loss: 1.4410099172694548\n",
      "Epoch 28/100, Loss: 1.4358651285420307\n",
      "Epoch 29/100, Loss: 1.4308023030397787\n",
      "Epoch 30/100, Loss: 1.425819847258661\n",
      "Epoch 31/100, Loss: 1.4209161947015865\n",
      "Epoch 32/100, Loss: 1.4160898056463844\n",
      "Epoch 33/100, Loss: 1.4113391669120274\n",
      "Epoch 34/100, Loss: 1.406662791622756\n",
      "Epoch 35/100, Loss: 1.4020592189697765\n",
      "Epoch 36/100, Loss: 1.3975270139702254\n",
      "Epoch 37/100, Loss: 1.3930647672231276\n",
      "Epoch 38/100, Loss: 1.388671094662107\n",
      "Epoch 39/100, Loss: 1.3843446373046482\n",
      "Epoch 40/100, Loss: 1.3800840609977363\n",
      "Epoch 41/100, Loss: 1.3758880561597546\n",
      "Epoch 42/100, Loss: 1.371755337518544\n",
      "Epoch 43/100, Loss: 1.3676846438455807\n",
      "Epoch 44/100, Loss: 1.3636747376862517\n",
      "Epoch 45/100, Loss: 1.3597244050862556\n",
      "Epoch 46/100, Loss: 1.3558324553141854\n",
      "Epoch 47/100, Loss: 1.3519977205803801\n",
      "Epoch 48/100, Loss: 1.3482190557521658\n",
      "Epoch 49/100, Loss: 1.3444953380656322\n",
      "Epoch 50/100, Loss: 1.3408254668341189\n",
      "Epoch 51/100, Loss: 1.3372083631535996\n",
      "Epoch 52/100, Loss: 1.3336429696051852\n",
      "Epoch 53/100, Loss: 1.3301282499549698\n",
      "Epoch 54/100, Loss: 1.3266631888514706\n",
      "Epoch 55/100, Loss: 1.323246791520916\n",
      "Epoch 56/100, Loss: 1.3198780834606514\n",
      "Epoch 57/100, Loss: 1.3165561101309327\n",
      "Epoch 58/100, Loss: 1.3132799366453962\n",
      "Epoch 59/100, Loss: 1.3100486474604796\n",
      "Epoch 60/100, Loss: 1.3068613460640814\n",
      "Epoch 61/100, Loss: 1.3037171546637443\n",
      "Epoch 62/100, Loss: 1.3006152138746443\n",
      "Epoch 63/100, Loss: 1.2975546824076598\n",
      "Epoch 64/100, Loss: 1.2945347367578017\n",
      "Epoch 65/100, Loss: 1.291554570893263\n",
      "Epoch 66/100, Loss: 1.2886133959453567\n",
      "Epoch 67/100, Loss: 1.285710439899589\n",
      "Epoch 68/100, Loss: 1.2828449472881154\n",
      "Epoch 69/100, Loss: 1.2800161788838091\n",
      "Epoch 70/100, Loss: 1.2772234113961722\n",
      "Epoch 71/100, Loss: 1.2744659371692992\n",
      "Epoch 72/100, Loss: 1.2717430638820968\n",
      "Epoch 73/100, Loss: 1.2690541142509555\n",
      "Epoch 74/100, Loss: 1.2663984257350491\n",
      "Epoch 75/100, Loss: 1.263775350244438\n",
      "Epoch 76/100, Loss: 1.2611842538511273\n",
      "Epoch 77/100, Loss: 1.2586245165032377\n",
      "Epoch 78/100, Loss: 1.2560955317424145\n",
      "Epoch 79/100, Loss: 1.253596706424609\n",
      "Epoch 80/100, Loss: 1.2511274604443425\n",
      "Epoch 81/100, Loss: 1.2486872264625588\n",
      "Epoch 82/100, Loss: 1.246275449638159\n",
      "Epoch 83/100, Loss: 1.2438915873633025\n",
      "Epoch 84/100, Loss: 1.241535109002551\n",
      "Epoch 85/100, Loss: 1.2392054956359158\n",
      "Epoch 86/100, Loss: 1.2369022398058704\n",
      "Epoch 87/100, Loss: 1.2346248452683746\n",
      "Epoch 88/100, Loss: 1.2323728267479448\n",
      "Epoch 89/100, Loss: 1.2301457096968123\n",
      "Epoch 90/100, Loss: 1.2279430300581835\n",
      "Epoch 91/100, Loss: 1.2257643340336277\n",
      "Epoch 92/100, Loss: 1.2236091778545979\n",
      "Epoch 93/100, Loss: 1.221477127558091\n",
      "Epoch 94/100, Loss: 1.219367758766447\n",
      "Epoch 95/100, Loss: 1.2172806564712793\n",
      "Epoch 96/100, Loss: 1.2152154148215224\n",
      "Epoch 97/100, Loss: 1.2131716369155818\n",
      "Epoch 98/100, Loss: 1.2111489345975648\n",
      "Epoch 99/100, Loss: 1.209146928257566\n",
      "Epoch 100/100, Loss: 1.207165246635976\n",
      "Accuracy: 0.7\n",
      "==================================================\n",
      "Quantization Level: 8\n",
      "Epoch 1/100, Loss: 1.791759469228055\n",
      "Epoch 2/100, Loss: 1.7842211609764924\n",
      "Epoch 3/100, Loss: 1.7767883945087497\n",
      "Epoch 4/100, Loss: 1.7694596481179083\n",
      "Epoch 5/100, Loss: 1.7622334164864106\n",
      "Epoch 6/100, Loss: 1.7551082106344253\n",
      "Epoch 7/100, Loss: 1.748082557858347\n",
      "Epoch 8/100, Loss: 1.7411550016610193\n",
      "Epoch 9/100, Loss: 1.7343241016751563\n",
      "Epoch 10/100, Loss: 1.727588433581282\n",
      "Epoch 11/100, Loss: 1.7209465890213855\n",
      "Epoch 12/100, Loss: 1.7143971755093634\n",
      "Epoch 13/100, Loss: 1.7079388163391933\n",
      "Epoch 14/100, Loss: 1.7015701504916798\n",
      "Epoch 15/100, Loss: 1.6952898325404973\n",
      "Epoch 16/100, Loss: 1.6890965325581444\n",
      "Epoch 17/100, Loss: 1.682988936022347\n",
      "Epoch 18/100, Loss: 1.676965743723342\n",
      "Epoch 19/100, Loss: 1.6710256716723921\n",
      "Epoch 20/100, Loss: 1.66516745101181\n",
      "Epoch 21/100, Loss: 1.659389827926694\n",
      "Epoch 22/100, Loss: 1.653691563558518\n",
      "Epoch 23/100, Loss: 1.6480714339206486\n",
      "Epoch 24/100, Loss: 1.6425282298158286\n",
      "Epoch 25/100, Loss: 1.637060756755595\n",
      "Epoch 26/100, Loss: 1.6316678348815814\n",
      "Epoch 27/100, Loss: 1.6263482988885989\n",
      "Epoch 28/100, Loss: 1.6211009979493771\n",
      "Epoch 29/100, Loss: 1.6159247956407996\n",
      "Epoch 30/100, Loss: 1.6108185698714699\n",
      "Epoch 31/100, Loss: 1.6057812128104054\n",
      "Epoch 32/100, Loss: 1.6008116308166556\n",
      "Epoch 33/100, Loss: 1.5959087443696351\n",
      "Epoch 34/100, Loss: 1.5910714879999361\n",
      "Epoch 35/100, Loss: 1.5862988102204072\n",
      "Epoch 36/100, Loss: 1.5815896734572656\n",
      "Epoch 37/100, Loss: 1.5769430539810259\n",
      "Epoch 38/100, Loss: 1.5723579418370168\n",
      "Epoch 39/100, Loss: 1.5678333407752885\n",
      "Epoch 40/100, Loss: 1.5633682681796945\n",
      "Epoch 41/100, Loss: 1.5589617549959613\n",
      "Epoch 42/100, Loss: 1.5546128456585648\n",
      "Epoch 43/100, Loss: 1.5503205980162407\n",
      "Epoch 44/100, Loss: 1.5460840832559726\n",
      "Epoch 45/100, Loss: 1.5419023858253222\n",
      "Epoch 46/100, Loss: 1.5377746033529633\n",
      "Epoch 47/100, Loss: 1.5336998465673164\n",
      "Epoch 48/100, Loss: 1.5296772392131763\n",
      "Epoch 49/100, Loss: 1.5257059179662593\n",
      "Epoch 50/100, Loss: 1.5217850323455915\n",
      "Epoch 51/100, Loss: 1.5179137446236903\n",
      "Epoch 52/100, Loss: 1.514091229734498\n",
      "Epoch 53/100, Loss: 1.5103166751790353\n",
      "Epoch 54/100, Loss: 1.5065892809287735\n",
      "Epoch 55/100, Loss: 1.5029082593267116\n",
      "Epoch 56/100, Loss: 1.499272834986178\n",
      "Epoch 57/100, Loss: 1.4956822446873794\n",
      "Epoch 58/100, Loss: 1.4921357372717274\n",
      "Epoch 59/100, Loss: 1.4886325735339871\n",
      "Epoch 60/100, Loss: 1.485172026112305\n",
      "Epoch 61/100, Loss: 1.481753379376169\n",
      "Epoch 62/100, Loss: 1.4783759293123824\n",
      "Epoch 63/100, Loss: 1.4750389834091127\n",
      "Epoch 64/100, Loss: 1.4717418605381112\n",
      "Epoch 65/100, Loss: 1.4684838908351845\n",
      "Epoch 66/100, Loss: 1.465264415579011\n",
      "Epoch 67/100, Loss: 1.4620827870684023\n",
      "Epoch 68/100, Loss: 1.458938368498108\n",
      "Epoch 69/100, Loss: 1.4558305338332684\n",
      "Epoch 70/100, Loss: 1.4527586676826183\n",
      "Epoch 71/100, Loss: 1.449722165170554\n",
      "Epoch 72/100, Loss: 1.4467204318081675\n",
      "Epoch 73/100, Loss: 1.4437528833633597\n",
      "Epoch 74/100, Loss: 1.440818945730142\n",
      "Epoch 75/100, Loss: 1.4379180547972314\n",
      "Epoch 76/100, Loss: 1.4350496563160562\n",
      "Epoch 77/100, Loss: 1.4322132057682684\n",
      "Epoch 78/100, Loss: 1.4294081682328796\n",
      "Epoch 79/100, Loss: 1.4266340182531152\n",
      "Epoch 80/100, Loss: 1.423890239703097\n",
      "Epoch 81/100, Loss: 1.4211763256544485\n",
      "Epoch 82/100, Loss: 1.4184917782429236\n",
      "Epoch 83/100, Loss: 1.4158361085351514\n",
      "Epoch 84/100, Loss: 1.413208836395591\n",
      "Epoch 85/100, Loss: 1.410609490353784\n",
      "Epoch 86/100, Loss: 1.4080376074719907\n",
      "Epoch 87/100, Loss: 1.4054927332132925\n",
      "Epoch 88/100, Loss: 1.40297442131024\n",
      "Epoch 89/100, Loss: 1.4004822336341243\n",
      "Epoch 90/100, Loss: 1.3980157400649396\n",
      "Epoch 91/100, Loss: 1.3955745183621113\n",
      "Epoch 92/100, Loss: 1.3931581540360536\n",
      "Epoch 93/100, Loss: 1.390766240220615\n",
      "Epoch 94/100, Loss: 1.3883983775464777\n",
      "Epoch 95/100, Loss: 1.38605417401556\n",
      "Epoch 96/100, Loss: 1.3837332448764772\n",
      "Epoch 97/100, Loss: 1.3814352125011078\n",
      "Epoch 98/100, Loss: 1.3791597062623122\n",
      "Epoch 99/100, Loss: 1.3769063624128453\n",
      "Epoch 100/100, Loss: 1.3746748239655027\n",
      "Accuracy: 0.6125\n",
      "==================================================\n",
      "Quantization Level: 9\n",
      "Epoch 1/100, Loss: 1.945910149055314\n",
      "Epoch 2/100, Loss: 1.9389050947981274\n",
      "Epoch 3/100, Loss: 1.93198450750603\n",
      "Epoch 4/100, Loss: 1.9251473823239267\n",
      "Epoch 5/100, Loss: 1.9183927235373823\n",
      "Epoch 6/100, Loss: 1.9117195444725765\n",
      "Epoch 7/100, Loss: 1.90512686739511\n",
      "Epoch 8/100, Loss: 1.898613723408361\n",
      "Epoch 9/100, Loss: 1.892179152352077\n",
      "Epoch 10/100, Loss: 1.8858222027017888\n",
      "Epoch 11/100, Loss: 1.8795419314696118\n",
      "Epoch 12/100, Loss: 1.873337404106931\n",
      "Epoch 13/100, Loss: 1.8672076944094123\n",
      "Epoch 14/100, Loss: 1.8611518844247428\n",
      "Epoch 15/100, Loss: 1.8551690643634402\n",
      "Epoch 16/100, Loss: 1.8492583325130354\n",
      "Epoch 17/100, Loss: 1.843418795155877\n",
      "Epoch 18/100, Loss: 1.8376495664907715\n",
      "Epoch 19/100, Loss: 1.8319497685586192\n",
      "Epoch 20/100, Loss: 1.8263185311721835\n",
      "Epoch 21/100, Loss: 1.8207549918500654\n",
      "Epoch 22/100, Loss: 1.8152582957549541\n",
      "Epoch 23/100, Loss: 1.8098275956361625\n",
      "Epoch 24/100, Loss: 1.8044620517764323\n",
      "Epoch 25/100, Loss: 1.7991608319429784\n",
      "Epoch 26/100, Loss: 1.7939231113426892\n",
      "Epoch 27/100, Loss: 1.7887480725814044\n",
      "Epoch 28/100, Loss: 1.7836349056271366\n",
      "Epoch 29/100, Loss: 1.778582807777117\n",
      "Epoch 30/100, Loss: 1.7735909836284982\n",
      "Epoch 31/100, Loss: 1.7686586450525468\n",
      "Epoch 32/100, Loss: 1.7637850111721387\n",
      "Epoch 33/100, Loss: 1.7589693083423668\n",
      "Epoch 34/100, Loss: 1.754210770134042\n",
      "Epoch 35/100, Loss: 1.7495086373198803\n",
      "Epoch 36/100, Loss: 1.7448621578631527\n",
      "Epoch 37/100, Loss: 1.74027058690857\n",
      "Epoch 38/100, Loss: 1.7357331867751689\n",
      "Epoch 39/100, Loss: 1.7312492269509796\n",
      "Epoch 40/100, Loss: 1.7268179840892295\n",
      "Epoch 41/100, Loss: 1.722438742005864\n",
      "Epoch 42/100, Loss: 1.7181107916781517\n",
      "Epoch 43/100, Loss: 1.713833431244152\n",
      "Epoch 44/100, Loss: 1.7096059660028282\n",
      "Epoch 45/100, Loss: 1.7054277084145872\n",
      "Epoch 46/100, Loss: 1.7012979781020505\n",
      "Epoch 47/100, Loss: 1.6972161018508423\n",
      "Epoch 48/100, Loss: 1.6931814136102232\n",
      "Epoch 49/100, Loss: 1.6891932544933657\n",
      "Epoch 50/100, Loss: 1.6852509727771192\n",
      "Epoch 51/100, Loss: 1.6813539239010857\n",
      "Epoch 52/100, Loss: 1.6775014704658626\n",
      "Epoch 53/100, Loss: 1.6736929822303026\n",
      "Epoch 54/100, Loss: 1.669927836107663\n",
      "Epoch 55/100, Loss: 1.6662054161605184\n",
      "Epoch 56/100, Loss: 1.6625251135943249\n",
      "Epoch 57/100, Loss: 1.658886326749537\n",
      "Epoch 58/100, Loss: 1.6552884610921832\n",
      "Epoch 59/100, Loss: 1.651730929202817\n",
      "Epoch 60/100, Loss: 1.648213150763779\n",
      "Epoch 61/100, Loss: 1.644734552544702\n",
      "Epoch 62/100, Loss: 1.6412945683862112\n",
      "Epoch 63/100, Loss: 1.6378926391817763\n",
      "Epoch 64/100, Loss: 1.6345282128576795\n",
      "Epoch 65/100, Loss: 1.6312007443510796\n",
      "Epoch 66/100, Loss: 1.6279096955861507\n",
      "Epoch 67/100, Loss: 1.624654535448288\n",
      "Epoch 68/100, Loss: 1.6214347397563826\n",
      "Epoch 69/100, Loss: 1.6182497912331655\n",
      "Epoch 70/100, Loss: 1.6150991794736367\n",
      "Epoch 71/100, Loss: 1.6119824009115973\n",
      "Epoch 72/100, Loss: 1.6088989587843117\n",
      "Epoch 73/100, Loss: 1.605848363095319\n",
      "Epoch 74/100, Loss: 1.6028301305754469\n",
      "Epoch 75/100, Loss: 1.5998437846420457\n",
      "Epoch 76/100, Loss: 1.5968888553565062\n",
      "Epoch 77/100, Loss: 1.5939648793800922\n",
      "Epoch 78/100, Loss: 1.591071399928148\n",
      "Epoch 79/100, Loss: 1.5882079667227316\n",
      "Epoch 80/100, Loss: 1.585374135943729\n",
      "Epoch 81/100, Loss: 1.5825694701785094\n",
      "Epoch 82/100, Loss: 1.5797935383701796\n",
      "Epoch 83/100, Loss: 1.5770459157645036\n",
      "Epoch 84/100, Loss: 1.5743261838555465\n",
      "Epoch 85/100, Loss: 1.5716339303301081\n",
      "Epoch 86/100, Loss: 1.5689687490110158\n",
      "Epoch 87/100, Loss: 1.5663302397993353\n",
      "Epoch 88/100, Loss: 1.56371800861557\n",
      "Epoch 89/100, Loss: 1.5611316673399158\n",
      "Epoch 90/100, Loss: 1.5585708337516333\n",
      "Epoch 91/100, Loss: 1.556035131467604\n",
      "Epoch 92/100, Loss: 1.5535241898801402\n",
      "Epoch 93/100, Loss: 1.5510376440941078\n",
      "Epoch 94/100, Loss: 1.5485751348634227\n",
      "Epoch 95/100, Loss: 1.546136308526996\n",
      "Epoch 96/100, Loss: 1.5437208169441716\n",
      "Epoch 97/100, Loss: 1.5413283174297336\n",
      "Epoch 98/100, Loss: 1.53895847268853\n",
      "Epoch 99/100, Loss: 1.536610950749774\n",
      "Epoch 100/100, Loss: 1.5342854249010827\n",
      "Accuracy: 0.625\n",
      "==================================================\n",
      "Quantization Level: 10\n",
      "Epoch 1/100, Loss: 1.945910149055314\n",
      "Epoch 2/100, Loss: 1.9397535087855808\n",
      "Epoch 3/100, Loss: 1.9336734638007929\n",
      "Epoch 4/100, Loss: 1.9276690435795043\n",
      "Epoch 5/100, Loss: 1.9217392848671118\n",
      "Epoch 6/100, Loss: 1.915883231819177\n",
      "Epoch 7/100, Loss: 1.9100999361345892\n",
      "Epoch 8/100, Loss: 1.9043884571789598\n",
      "Epoch 9/100, Loss: 1.8987478620986753\n",
      "Epoch 10/100, Loss: 1.8931772259259934\n",
      "Epoch 11/100, Loss: 1.8876756316755734\n",
      "Epoch 12/100, Loss: 1.8822421704328303\n",
      "Epoch 13/100, Loss: 1.876875941434473\n",
      "Epoch 14/100, Loss: 1.8715760521415938\n",
      "Epoch 15/100, Loss: 1.866341618305658\n",
      "Epoch 16/100, Loss: 1.8611717640277163\n",
      "Epoch 17/100, Loss: 1.8560656218111824\n",
      "Epoch 18/100, Loss: 1.851022332608466\n",
      "Epoch 19/100, Loss: 1.8460410458617713\n",
      "Epoch 20/100, Loss: 1.8411209195383385\n",
      "Epoch 21/100, Loss: 1.836261120160395\n",
      "Epoch 22/100, Loss: 1.831460822830082\n",
      "Epoch 23/100, Loss: 1.8267192112495885\n",
      "Epoch 24/100, Loss: 1.822035477736734\n",
      "Epoch 25/100, Loss: 1.817408823236216\n",
      "Epoch 26/100, Loss: 1.8128384573267204\n",
      "Epoch 27/100, Loss: 1.8083235982241086\n",
      "Epoch 28/100, Loss: 1.803863472780845\n",
      "Epoch 29/100, Loss: 1.7994573164818533\n",
      "Epoch 30/100, Loss: 1.7951043734369612\n",
      "Epoch 31/100, Loss: 1.7908038963700839\n",
      "Epoch 32/100, Loss: 1.7865551466052978\n",
      "Epoch 33/100, Loss: 1.782357394049935\n",
      "Epoch 34/100, Loss: 1.7782099171748353\n",
      "Epoch 35/100, Loss: 1.7741120029918669\n",
      "Epoch 36/100, Loss: 1.7700629470288398\n",
      "Epoch 37/100, Loss: 1.7660620533019145\n",
      "Epoch 38/100, Loss: 1.7621086342856096\n",
      "Epoch 39/100, Loss: 1.7582020108805037\n",
      "Epoch 40/100, Loss: 1.7543415123787274\n",
      "Epoch 41/100, Loss: 1.7505264764273263\n",
      "Epoch 42/100, Loss: 1.7467562489895827\n",
      "Epoch 43/100, Loss: 1.743030184304373\n",
      "Epoch 44/100, Loss: 1.7393476448436378\n",
      "Epoch 45/100, Loss: 1.7357080012680346\n",
      "Epoch 46/100, Loss: 1.7321106323808457\n",
      "Epoch 47/100, Loss: 1.728554925080207\n",
      "Epoch 48/100, Loss: 1.7250402743097244\n",
      "Epoch 49/100, Loss: 1.721566083007535\n",
      "Epoch 50/100, Loss: 1.7181317620538867\n",
      "Epoch 51/100, Loss: 1.7147367302172782\n",
      "Epoch 52/100, Loss: 1.711380414099235\n",
      "Epoch 53/100, Loss: 1.7080622480777692\n",
      "Epoch 54/100, Loss: 1.7047816742495765\n",
      "Epoch 55/100, Loss: 1.701538142371033\n",
      "Epoch 56/100, Loss: 1.6983311097980411\n",
      "Epoch 57/100, Loss: 1.6951600414247796\n",
      "Epoch 58/100, Loss: 1.6920244096214083\n",
      "Epoch 59/100, Loss: 1.6889236941707786\n",
      "Epoch 60/100, Loss: 1.6858573822042082\n",
      "Epoch 61/100, Loss: 1.6828249681363594\n",
      "Epoch 62/100, Loss: 1.6798259535992792\n",
      "Epoch 63/100, Loss: 1.6768598473756509\n",
      "Epoch 64/100, Loss: 1.6739261653312998\n",
      "Epoch 65/100, Loss: 1.6710244303470076\n",
      "Epoch 66/100, Loss: 1.6681541722496824\n",
      "Epoch 67/100, Loss: 1.665314927742927\n",
      "Epoch 68/100, Loss: 1.6625062403370614\n",
      "Epoch 69/100, Loss: 1.659727660278635\n",
      "Epoch 70/100, Loss: 1.6569787444794852\n",
      "Epoch 71/100, Loss: 1.6542590564453774\n",
      "Epoch 72/100, Loss: 1.6515681662042787\n",
      "Epoch 73/100, Loss: 1.648905650234304\n",
      "Epoch 74/100, Loss: 1.646271091391382\n",
      "Epoch 75/100, Loss: 1.6436640788366774\n",
      "Epoch 76/100, Loss: 1.641084207963818\n",
      "Epoch 77/100, Loss: 1.6385310803259607\n",
      "Epoch 78/100, Loss: 1.6360043035627452\n",
      "Epoch 79/100, Loss: 1.63350349132716\n",
      "Epoch 80/100, Loss: 1.6310282632123787\n",
      "Epoch 81/100, Loss: 1.6285782446785846\n",
      "Epoch 82/100, Loss: 1.6261530669798323\n",
      "Epoch 83/100, Loss: 1.6237523670909813\n",
      "Epoch 84/100, Loss: 1.6213757876347248\n",
      "Epoch 85/100, Loss: 1.6190229768087658\n",
      "Epoch 86/100, Loss: 1.6166935883131537\n",
      "Epoch 87/100, Loss: 1.6143872812778277\n",
      "Epoch 88/100, Loss: 1.6121037201903916\n",
      "Epoch 89/100, Loss: 1.6098425748241474\n",
      "Epoch 90/100, Loss: 1.6076035201664198\n",
      "Epoch 91/100, Loss: 1.605386236347199\n",
      "Epoch 92/100, Loss: 1.6031904085681241\n",
      "Epoch 93/100, Loss: 1.6010157270318417\n",
      "Epoch 94/100, Loss: 1.59886188687175\n",
      "Epoch 95/100, Loss: 1.5967285880821638\n",
      "Epoch 96/100, Loss: 1.5946155354489198\n",
      "Epoch 97/100, Loss: 1.592522438480437\n",
      "Epoch 98/100, Loss: 1.590449011339262\n",
      "Epoch 99/100, Loss: 1.5883949727741107\n",
      "Epoch 100/100, Loss: 1.5863600460524303\n",
      "Accuracy: 0.475\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Logistic_question.csv\")\n",
    "\n",
    "# Extract features and target\n",
    "X = data.drop(columns=['Target']).values\n",
    "y_true = data['Target'].values\n",
    "\n",
    "def quantize_target(y_true, num_levels):\n",
    "    \"\"\"Quantize the target column into the specified number of levels.\"\"\"\n",
    "    quantized_targets = np.linspace(0, np.max(y_true), num_levels)\n",
    "    return np.digitize(y_true, quantized_targets)\n",
    "\n",
    "\n",
    "for num_levels in range(2, 11):\n",
    "    print(f\"Quantization Level: {num_levels}\")\n",
    "    y_quantized = quantize_target(y_true, num_levels)\n",
    "    \n",
    "    # One-hot encode the quantized target\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    y_onehot = encoder.fit_transform(y_quantized.reshape(-1, 1))\n",
    "    \n",
    "    # Determine the number of classes\n",
    "    num_classes = y_onehot.shape[1]\n",
    "    \n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train the Multinomial Logistic Regression model\n",
    "    model = MultinomialLogisticRegression(num_features=X_train_scaled.shape[1], num_classes=num_classes)\n",
    "    model.train(X_train_scaled, y_train, num_epochs=100)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of2sHl5Z4dXi"
   },
   "source": [
    "**Question:** Report for which $i$ your model performs best. Describe and analyze the results! You could use visualizations or any other method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRLERDAr4wnS"
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wT43jGKV6CBZ"
   },
   "source": [
    "# Going a little further!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo9uGo0R6GZo"
   },
   "source": [
    "First we download Adult income dataset from Kaggle! In order to do this create an account on this website, and create an API. A file named kaggle.json will be downloaded to your device. Then use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "o-vrjYBF7u1E",
    "outputId": "b274bc6e-4c35-4ad8-f17b-9e69f7d92923"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      2\u001b[0m files\u001b[38;5;241m.\u001b[39mupload()  \u001b[38;5;66;03m# Use this to select the kaggle.json file from your computer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmkdir -p ~/.kaggle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()  # Use this to select the kaggle.json file from your computer\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i6u6_1v8ftX"
   },
   "source": [
    "Then use this code to automatically download the dataset into Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjyVaVKF29Hx",
    "outputId": "15d0b1a2-c806-4102-abbc-12545237e218"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d wenruliu/adult-income-dataset\n",
    "!unzip /content/adult-income-dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXQnbZwt8rJK"
   },
   "source": [
    "**Task:** Determine the number of null entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtuEx6QW29c1",
    "outputId": "43397bec-0622-4dc4-de2b-c65be00e4503",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6465\n",
      "       age     workclass  fnlwgt     education  educational-num  \\\n",
      "0       25       Private  226802          11th                7   \n",
      "1       38       Private   89814       HS-grad                9   \n",
      "2       28     Local-gov  336951    Assoc-acdm               12   \n",
      "3       44       Private  160323  Some-college               10   \n",
      "4       18           NaN  103497  Some-college               10   \n",
      "...    ...           ...     ...           ...              ...   \n",
      "48837   27       Private  257302    Assoc-acdm               12   \n",
      "48838   40       Private  154374       HS-grad                9   \n",
      "48839   58       Private  151910       HS-grad                9   \n",
      "48840   22       Private  201490       HS-grad                9   \n",
      "48841   52  Self-emp-inc  287927       HS-grad                9   \n",
      "\n",
      "           marital-status         occupation relationship   race  gender  \\\n",
      "0           Never-married  Machine-op-inspct    Own-child  Black    Male   \n",
      "1      Married-civ-spouse    Farming-fishing      Husband  White    Male   \n",
      "2      Married-civ-spouse    Protective-serv      Husband  White    Male   \n",
      "3      Married-civ-spouse  Machine-op-inspct      Husband  Black    Male   \n",
      "4           Never-married                NaN    Own-child  White  Female   \n",
      "...                   ...                ...          ...    ...     ...   \n",
      "48837  Married-civ-spouse       Tech-support         Wife  White  Female   \n",
      "48838  Married-civ-spouse  Machine-op-inspct      Husband  White    Male   \n",
      "48839             Widowed       Adm-clerical    Unmarried  White  Female   \n",
      "48840       Never-married       Adm-clerical    Own-child  White    Male   \n",
      "48841  Married-civ-spouse    Exec-managerial         Wife  White  Female   \n",
      "\n",
      "       capital-gain  capital-loss  hours-per-week native-country income  \n",
      "0                 0             0              40  United-States  <=50K  \n",
      "1                 0             0              50  United-States  <=50K  \n",
      "2                 0             0              40  United-States   >50K  \n",
      "3              7688             0              40  United-States   >50K  \n",
      "4                 0             0              30  United-States  <=50K  \n",
      "...             ...           ...             ...            ...    ...  \n",
      "48837             0             0              38  United-States  <=50K  \n",
      "48838             0             0              40  United-States   >50K  \n",
      "48839             0             0              40  United-States  <=50K  \n",
      "48840             0             0              20  United-States  <=50K  \n",
      "48841         15024             0              40  United-States   >50K  \n",
      "\n",
      "[48842 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here!\n",
    "df = pd.read_csv(\"adult1.csv\", na_values='?', delimiter='\u001b')\n",
    "print(df.isnull().sum().sum())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpEcBdTUAYVN"
   },
   "source": [
    "**Question:** In many widely used datasets there are a lot of null entries. Propose 5 methods by which, one could deal with this problem. Briefly explain how do you decide which one to use in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1u1pBHuAsSg"
   },
   "source": [
    "**Your answer:**\n",
    "### Dealing with Null Entries in Datasets\n",
    "\n",
    "\n",
    "1. **Dropping Rows/Columns with Null Values**:\n",
    "   - *Method*: Remove rows or columns containing null values from the dataset.\n",
    "   - *Decision*: Suitable when null values are sporadic and do not significantly impact the dataset's overall information.\n",
    "\n",
    "\n",
    "2. **Imputation**:\n",
    "   - *Method*: Fill null values with a statistical measure like mean, median, mode, or using advanced techniques like K-Nearest Neighbors (KNN) imputation.\n",
    "   - *Decision*: Effective when null values are random and can be reasonably estimated from existing data.\n",
    "\n",
    "\n",
    "3. **Forward/Backward Fill**:\n",
    "   - *Method*: Fill null values with the preceding (forward fill) or succeeding (backward fill) non-null value in the same column.\n",
    "   - *Decision*: Applicable when null values represent a continuous sequence and can be reasonably filled using adjacent values.\n",
    "\n",
    "\n",
    "4. **Predictive Imputation**:\n",
    "   - *Method*: Utilize machine learning algorithms to predict and fill null values based on the remaining features.\n",
    "   - *Decision*: Useful when null values are not missing at random and have a pattern that can be learned from other features.\n",
    "\n",
    "\n",
    "5. **Creating a Separate Category for Nulls**:\n",
    "   - *Method*: Encode null values as a separate category to preserve information.\n",
    "   - *Decision*: Suitable when null values contain meaningful information or when imputation may introduce bias.\n",
    "\n",
    "### Choosing the Suitable Method\n",
    "\n",
    "\n",
    "- **Data Distribution**: Analyze null value distribution to understand randomness or patterns.\n",
    "- **Impact on Analysis**: Evaluate how null values affect analysis tasks.\n",
    "\n",
    "- **Domain Knowledge**: Consider domain-specific insights for handling null values.\n",
    "- **Dataset Size**: Take dataset size and null value proportions into account for effective handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHhH-hkpAxFf"
   },
   "source": [
    "**Task:** Handle null entries using your best method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5fVwWcjK29fk",
    "outputId": "c21a6adf-1e6c-46d0-dd61-79d1710272c1"
   },
   "outputs": [],
   "source": [
    "# Your code goes here!\n",
    "df = df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43k5cTorCJaV"
   },
   "source": [
    "**Task:** Convert categorical features to numerical values. Split the dataset with 80-20 portion. Normalize all the data using X_train. Use the built-in Logistic Regression function and GridSearchCV to train your model, and report the parameters, train and test accuracy of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Agj18Lcd-vyZ",
    "outputId": "69e132a9-0249-4a21-c8f3-45247c1e17dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Train Accuracy: 0.8201895001686644\n",
      "Test Accuracy: 0.8208955223880597\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def extract_categorical_features(df, target_column, max_unique_values=20):\n",
    "    \"\"\"\n",
    "    Extracts the categorical features from the given DataFrame based on\n",
    "    the data type and the number of unique values.\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    for column in df.columns:\n",
    "        if column != target_column:\n",
    "            if df[column].dtype == 'object' or df[column].nunique() <= max_unique_values:\n",
    "                categorical_features.append(column)\n",
    "    return categorical_features\n",
    "\n",
    "def preprocess_data(df, target_column, categorical_features):\n",
    "    \"\"\"\n",
    "    Encodes categorical features, splits the dataset, and normalizes the features.\n",
    "    \"\"\"\n",
    "    # Encode categorical features\n",
    "    label_encoders = {}\n",
    "    for feature in categorical_features:\n",
    "        label_encoders[feature] = LabelEncoder()\n",
    "        df[feature] = label_encoders[feature].fit_transform(df[feature])\n",
    "    \n",
    "    # Convert target column to binary\n",
    "    df[target_column] = LabelEncoder().fit_transform(df[target_column])\n",
    "\n",
    "    # Split the dataset into features and target\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Normalize the data using X_train\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "def train_evaluate_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains the Logistic Regression model using GridSearchCV and evaluates it.\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "\n",
    "    # Create the Logistic Regression model and perform GridSearchCV\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and the train and test accuracy\n",
    "    best_parameters = grid_search.best_params_\n",
    "    train_accuracy = grid_search.best_score_\n",
    "    test_accuracy = grid_search.score(X_test, y_test)\n",
    "    \n",
    "    return best_parameters, train_accuracy, test_accuracy\n",
    "\n",
    "# Assuming df is your dataframe and 'income' is your target column\n",
    "categorical_features = extract_categorical_features(df, 'income')\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(df, 'income', categorical_features)\n",
    "best_params, train_acc, test_acc = train_evaluate_model(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Lzr2lqXDQ1T"
   },
   "source": [
    "**Task:** To try a different route, split X_train into $i$ parts, and train $i$ separate models on these parts. Now propose and implement 3 different *ensemble methods* to derive the global models' prediction for X_test using the results(not necessarily predictions!) of the $i$ models. Firstly, set $i=10$ to find the method with the best test accuracy(the answer is not general!). You must Use your own Logistic Regression model.(You might want to modify it a little bit for this part!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "K9D1jlstF9nF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy: 0.125\n",
      "Average Probability Accuracy: 0.7625\n",
      "Weighted Average Accuracy: 0.7125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class MyLogisticRegressionEnsemble(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([MyLogisticRegression(input_size) for _ in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [model(x) for model in self.models]\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "    def fit(self, X_train_parts, y_train_parts, epochs, lr):\n",
    "        for model, X_train, y_train in zip(self.models, X_train_parts, y_train_parts):\n",
    "            train_data = torch.utils.data.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train).unsqueeze(1))\n",
    "            train_loader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
    "            model.fit(train_loader, epochs, lr)\n",
    "\n",
    "    def ensemble_majority_vote(self, X_test):\n",
    "        y_pred_list = [model.predict(X_test) for model in self.models]\n",
    "        flat_predictions = [pred.flatten().astype(int) for pred in y_pred_list]\n",
    "        y_pred_ensemble = np.array([np.argmax(np.bincount(pred)) for pred in zip(*flat_predictions)])\n",
    "        return y_pred_ensemble\n",
    "\n",
    "    def ensemble_average_probability(self, X_test):\n",
    "        y_pred_list = [model.predict(X_test) for model in self.models]\n",
    "        y_pred_ensemble = np.mean(y_pred_list, axis=0)\n",
    "        y_pred_ensemble = [1 if prob > 0.5 else 0 for prob in y_pred_ensemble]\n",
    "        return y_pred_ensemble\n",
    "\n",
    "    def ensemble_weighted_average(self, X_test):\n",
    "        y_pred_list = [model.predict(X_test) for model in self.models]\n",
    "        weights = [1 / (i + 1) for i in range(10)]\n",
    "        y_pred_ensemble = np.average(y_pred_list, axis=0, weights=weights)\n",
    "        y_pred_ensemble = [1 if prob > 0.5 else 0 for prob in y_pred_ensemble]\n",
    "        return y_pred_ensemble\n",
    "\n",
    "class MyLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, pred, target):\n",
    "        return nn.functional.binary_cross_entropy(pred, target)\n",
    "\n",
    "    def fit(self, train_loader, epochs, lr):\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            for data, target in train_loader:\n",
    "                pred = self(data)\n",
    "                loss = self.loss(pred, target)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        pred = self(torch.Tensor(test_data).float())\n",
    "        return pred.detach().numpy()\n",
    "\n",
    "# Assuming X_train_parts and y_train_parts are lists of training data splits\n",
    "# Normalize the data using X_train data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the ensemble model\n",
    "ensemble_model = MyLogisticRegressionEnsemble(X_train.shape[1])\n",
    "ensemble_model.fit(X_train_parts, y_train_parts, epochs=10, lr=0.01)\n",
    "\n",
    "# Evaluate the ensemble methods\n",
    "print(\"Majority Vote Accuracy:\", accuracy_score(y_test, ensemble_model.ensemble_majority_vote(X_test)))\n",
    "print(\"Average Probability Accuracy:\", accuracy_score(y_test, ensemble_model.ensemble_average_probability(X_test)))\n",
    "print(\"Weighted Average Accuracy:\", accuracy_score(y_test, ensemble_model.ensemble_weighted_average(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QS9HYJ5FW1T"
   },
   "source": [
    "**Question:** Explain your proposed methods and the reason you decided to use them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hCBQuAeF46a"
   },
   "source": [
    "**Your answer:**\n",
    "1. Majority Vote Ensemble\n",
    "Method Description:\n",
    "Each individual logistic regression model in the ensemble predicts the class label (0 or 1) for a given input.\n",
    "The ensemble combines these predictions by selecting the class label that receives the majority of votes across all models.\n",
    "Reasoning:\n",
    "Majority voting helps mitigate the impact of individual model biases.\n",
    "It leverages the collective wisdom of multiple models, potentially improving overall accuracy.\n",
    "It is robust to noisy predictions from individual models.\n",
    "Implementation:\n",
    "The ensemble_majority_vote method computes the majority vote prediction for the test data.\n",
    "2. Average Probability Ensemble\n",
    "Method Description:\n",
    "Each individual logistic regression model predicts the probability of the positive class (class 1) for each input.\n",
    "The ensemble computes the average probability across all models.\n",
    "A threshold (usually 0.5) is applied to convert probabilities into binary class labels.\n",
    "Reasoning:\n",
    "Averaging probabilities provides a more nuanced view of uncertainty.\n",
    "It can capture subtle differences between models.\n",
    "Useful when individual models have varying confidence levels.\n",
    "Implementation:\n",
    "The ensemble_average_probability method calculates the average probability and converts it to class labels.\n",
    "3. Weighted Average Ensemble\n",
    "Method Description:\n",
    "Similar to the average probability ensemble, but with weighted averaging.\n",
    "Each model’s prediction is multiplied by a weight (e.g., inversely proportional to its index).\n",
    "The ensemble combines these weighted predictions.\n",
    "Reasoning:\n",
    "Weighted averaging allows emphasizing certain models over others.\n",
    "It accounts for the relative performance of individual models.\n",
    "Useful when some models are more reliable or informative.\n",
    "Implementation:\n",
    "The ensemble_weighted_average method computes the weighted average prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjSREvg4FTHf"
   },
   "source": [
    "**Task:** Now, for your best method, change $i$ from 2 to 100 and report $i$, train and test accuracy of the best model. Also, plot test and train accuracy for $2\\leq i\\leq100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "tfKS-Jq0-v4P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:\n",
      "i: 2\n",
      "Train accuracy: 0.078125\n",
      "Test accuracy: 0.125\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the ensemble class and logistic regression class (same as before)\n",
    "\n",
    "# Define a function to train and evaluate the ensemble model for a given value of i\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, i):\n",
    "    X_train_parts = np.array_split(X_train, i)\n",
    "    y_train_parts = np.array_split(y_train, i)\n",
    "\n",
    "    # Normalize the data using X_train data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Create and train the ensemble model\n",
    "    ensemble_model = MyLogisticRegressionEnsemble(X_train.shape[1])\n",
    "    ensemble_model.fit(X_train_parts, y_train_parts, epochs=10, lr=0.01)\n",
    "\n",
    "    # Evaluate the ensemble methods\n",
    "    train_accuracy = accuracy_score(y_train, ensemble_model.ensemble_majority_vote(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, ensemble_model.ensemble_majority_vote(X_test))\n",
    "    \n",
    "    return i, train_accuracy, test_accuracy\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "\n",
    "# Loop through values of i from 2 to 100\n",
    "for i in range(2, 101):\n",
    "    result = train_and_evaluate(X_train, X_test, y_train, y_test, i)\n",
    "    results.append(result)\n",
    "\n",
    "# Find the best model based on test accuracy\n",
    "best_model = max(results, key=lambda x: x[2])\n",
    "\n",
    "print(\"Best model:\")\n",
    "print(\"i:\", best_model[0])\n",
    "print(\"Train accuracy:\", best_model[1])\n",
    "print(\"Test accuracy:\", best_model[2])\n",
    "\n",
    "# Plot train and test accuracy for 2 <= i <= 100\n",
    "is_values = [result[0] for result in results]\n",
    "train_accuracies = [result[1] for result in results]\n",
    "test_accuracies = [result[2] for result in results]\n",
    "\n",
    "plt.plot(is_values, train_accuracies, label='Train Accuracy')\n",
    "plt.plot(is_values, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy vs. i')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWV0YUgRGg1p"
   },
   "source": [
    "**Question:** Analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
